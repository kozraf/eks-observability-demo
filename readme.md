# EKS Observability Demo

> **TL;DR** â€“ This repository bootstraps an AWS CodeBuild project (via GitHub Actions) that in turn provisions an EKS cluster with a fullyâ€‘remote Terraform backend (S3+DynamoDB) and deploys basic cost/observability addâ€‘ons (Kubecost, optional Prometheus Stack). The whole process is *clickâ€‘once*: run the **Bootstrap AWS CodeBuild** workflow and watch CodeBuild do the rest.

---

## Table of Contents

1. [Architecture](#architecture)
2. [Repository layout](#repository-layout)
3. [Prerequisites](#prerequisites)
4. [Secrets & environment variables](#secrets--environment-variables)
5. [Bootstrap phase](#bootstrap-phase)
6. [Remote backend](#remote-backend)
7. [EKS deployment phase](#eks-deployment-phase)
8. [Postâ€‘deploy addâ€‘ons](#post-deploy-add-ons)
9. [Using the cluster](#using-the-cluster)
10. [Troubleshooting](#troubleshooting)
11. [Cleanup / teardown](#cleanup--teardown)

---

## Architecture

Local Git repo -> GitHub Actions -> Bootstrap workflow (CI) -> AWS CodeBuild (CD)

Bootstrap workflow (CI)
* Creates CodeBuild project
* Creates S3 + DynamoDB (remote TF backend)
* Stores GH_PAT in SecretsMgr

AWS CodeBuild (CD)
buildspec.yml steps:
1.Init TF with remote
2.Apply EKS, VPC, etc.
3.aws eks updateâ€‘kubeconfig
4.deploy.sh (Helm addâ€‘ons)


---

## Repository layout

| Path                              | Purpose                                                                                                                                                       |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `.github/workflows/bootstrap.yml` | GitHubÂ Actions workflow that **bootstraps** the AWSÂ CodeBuild project and backend.                                                                            |
| `bootstrap/*`                     | Terraform that creates:<br>â€¢ the CodeBuild project/role<br>â€¢ the S3Â bucket + DynamoDB lock table (random name)<br>â€¢ stores your GitHubÂ PAT in SecretsÂ Manager |
| `terraform/`                      | Terraform that deploys the VPC, EKSâ€¯cluster, nodeÂ group, IAM, etc. (remote backend configured in `backend.tf`).                                               |
| `scripts/deploy.sh`               | Helm script that installs Kubecost (Prometheus stack commented out â€“ enable if desired).                                                                      |
| `buildspec.yml`                   | CodeBuild instruction file (installs TF/kubectl/Helm, then runs the EKS stack and addâ€‘ons).                                                                   |

---

## Prerequisites

* **AWSâ€¯Account** with Administrator (or least privileges to create EKS, IAM, S3, DynamoDB, CodeBuild, SecretsÂ Manager, VPC, CloudWatch).
* **GitHubâ€¯Secrets** (in the repo â†’ *SettingsÂ â†’ SecretsÂ &Â variablesÂ â†’ Actions*):

  * `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` â€“ credentials that GitHubÂ Actions uses for bootstrapping.
  * `GH_PAT` â€“ *classic* personal access token with at least **repo** scope so CodeBuild can pull the source.
* Terraform fileâ€‘backend **not required** locally â€“ everything is remote.

---

## SecretsÂ &Â environment variables

| Name                                          | Where                                              | UsedÂ by                               | Description                                      |
| --------------------------------------------- | -------------------------------------------------- | ------------------------------------- | ------------------------------------------------ |
| `AWS_REGION`                                  | workflow & CodeBuild env                           | All                                   | Deployment region (default `usâ€‘eastâ€‘1`).         |
| `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY` | GitHubÂ Secrets                                     | Bootstrap workflow                    | Temp creds for bootstrap.                        |
| `GH_PAT`                                      | GitHubÂ Secrets                                     | bootstrap/`aws_secretsmanager_secret` | Stored in SecretsÂ Manager for CodeBuild.         |
| `TF_STATE_BUCKET` / `TF_LOCK_TABLE`           | Exported from bootstrap TF outputs â†’ CodeBuild env | CodeBuild                             | Names of the remote state bucket and lock table. |

Bootstrap exposes the bucket/table via outputs; GitHubÂ Actions writes them into CodeBuild project environment variables so the `buildspec.yml` can feed them into `terraform init`.

---

## Bootstrap phase

1. Trigger **Actions â†’ *Bootstrap AWSÂ CodeBuild* â†’ *Run workflow***.
2. Steps executed:

   * Terraform `bootstrap/` is initialised **locally in GitHub Actions**.
   * Creates **randomâ€‘named** S3Â bucket (`eksâ€‘tfâ€‘stateâ€‘<rand>`) and DynamoDB table (`eksâ€‘tfâ€‘lockâ€‘<rand>`).
   * Provisions **CodeBuild project + IAM role** with permissions for EKS, VPC, IAM, S3, etc.
   * Stores your `GH_PAT` in AWSâ€¯SecretsÂ Manager so CodeBuild can gitâ€‘clone the repo.
3. Outputs bucket & table names; workflow patches the CodeBuild env so subsequent builds get `TF_STATE_BUCKET`, `TF_LOCK_TABLE`.

You run this only once (or when you change CodeBuild infrastructure).

---

## Remote backend

`terraform/backend.tf` (inside the `terraform/` folder) contains **only** the backendÂ block:

```hcl
terraform {
  backend "s3" {}
}
```

Backend details are injected at runtime by the `buildspec.yml`:

```bash
terraform -chdir=terraform init \
  -backend-config="bucket=$TF_STATE_BUCKET" \
  -backend-config="key=eks-observability/eks.tfstate" \
  -backend-config="region=$AWS_REGION" \
  -backend-config="dynamodb_table=$TF_LOCK_TABLE" \
  -backend-config="encrypt=true"
```

No state files land in the repo.

---

## EKS deployment phase

Every push (or manual run) of CodeBuild does:

1. **Install tooling** â€“ TerraformÂ 1.11+, kubectlÂ 1.30, HelmÂ 3.
2. **`terraform apply`** in `terraform/` â€“ components:

   * VPC (`terraform-awsâ€‘vpc` module)
   * EKSÂ cluster (`terraform-awsâ€‘eks` 20.x)
   * Single managed node group
   * OIDC provider & KMS key
   * IAMÂ role `eks-admin-role` mapped to `system:masters` via `enable_cluster_creator_admin_permissions = true`
3. **Kubeconfig** â€“ `aws eks updateâ€‘kubeconfig` writes `~/.kube/config` inside the CodeBuild container (not persisted) so Helm can deploy addâ€‘ons.
4. **Addâ€‘ons** â€“ `scripts/deploy.sh` installs Kubecost (Prometheus optional).

Approximate total build time Â±Â 25Â minutes.

---

## Postâ€‘deploy addâ€‘ons

| Addâ€‘on               | Namespace    | Access                                                                                                                                  |
| -------------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------------------- |
| **Kubecost**         | `kubecost`   | Portâ€‘forward: `kubectl port-forward -n kubecost deploy/kubecost-cost-analyzer 9090` then [http://localhost:9090](http://localhost:9090) |
| **PrometheusÂ Stack** | `monitoring` | (uncomment in `deploy.sh`)                                                                                                              |

> **Note** â€“ EKSÂ 1.23+ requires the EBSâ€‘CSI driver for PVs. Either install the managed addâ€‘on or use Helm ([https://github.com/kubernetes-sigs/aws-ebs-csi-driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)). Kubecost will warn if missing.

---

## Using the cluster

1. **Assume the CodeBuild role locally** (or any IAM principal that you map).
2. Retrieve kubeconfig:

   ```bash
   aws eks --region us-east-1 update-kubeconfig --name my-eks-cluster
   ```
3. Verify:

   ```bash
   kubectl get nodes
   kubectl get pods -A
   ```

If you see *â€œYou must be logged in to the server (Unauthorized)â€* ensure your IAMÂ principal is in the **EKS Access Entry** list **and** mapped in the `awsâ€‘auth`Â ConfigMap. The module does this automatically when `enable_cluster_creator_admin_permissions=true`.

---

## Troubleshooting

| Symptom                                                  | Fix                                                                                                                                                              |
| -------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Unauthorized** error in CodeBuild on `awsâ€‘auth` update | Make sure the CodeBuild role ARN is passed to `cluster_security_group_additional_rules` or `enable_cluster_creator_admin_permissions=true` (already set). Retry. |
| Kubecost shows PVC errors                                | Install the EBSâ€‘CSI driver: `eksctl create addon --name aws-ebs-csi-driver --cluster my-eks-cluster --service-account-role-arn <role>`                           |
| Stuck deleting S3Â bucket                                 | State files / versions exist; empty bucket or use `aws s3 rm --recursive`.                                                                                       |

---

## Cleanup / teardown

1. **Destroy the EKS stack:**

   ```bash
   aws codebuild start-build --project-name eks-observability-demo \
     --environment-variables-override name=TF_ACTION,value=destroy \
     --buildspec-override buildspec.yml
   ```

   (or change the `terraform apply` command to `destroy` and run once).
2. **Delete CodeBuild & backend:** reâ€‘run `bootstrap/` with `terraform destroy` *from your workstation* or delete via AWSÂ Console.

---

## â­Â Credits

* Modules â€“ [https://github.com/terraform-aws-modules](https://github.com/terraform-aws-modules)
* Kubecost â€“ [https://kubecost.com](https://kubecost.com)

Happy observabilityÂ ğŸ‘€ğŸš€
